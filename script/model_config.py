"""
Model configuration file
Contains configuration information for all supported models and client initialization logic
"""

import os
from typing import Dict, Any, Optional, List, Union
import openai
from openai import OpenAI
from anthropic import Anthropic

# Default API configuration
DEFAULT_API_KEY = os.getenv("OPENAI_API_KEY")
DEFAULT_BASE_URL = os.getenv("OPENAI_BASE_URL")

LOCAL_API_KEY = ""
LOCAL_BASE_URL = ""

CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY")
CLAUDE_BASE_URL = os.getenv("CLAUDE_BASE_URL")

AZURE_API_KEY = os.getenv("AZURE_API_KEY")
AZURE_BASE_URL = os.getenv("AZURE_BASE_URL")
AZURE_API_VERSION = os.getenv("AZURE_API_VERSION")
# Model configuration dictionary
MODEL_CONFIGS = {
    # OpenAI models
    "gpt-4o-2024-11-20": {
        "model_id": "gpt-4o-2024-11-20",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": DEFAULT_API_KEY,
        "base_url": DEFAULT_BASE_URL,
        "client": "openai",
    },
    "gpt-4o-mini-2024-07-18": {
        "model_id": "gpt-4o-mini-2024-07-18",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": DEFAULT_API_KEY,
        "base_url": DEFAULT_BASE_URL,
        "client": "openai",
    },
    "o4-mini-2025-04-16": {
        "model_id": "o4-mini-2025-04-16",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": DEFAULT_API_KEY,
        "base_url": DEFAULT_BASE_URL,
        "client": "openai",
    },
    "gpt-oss-120b": {
        "model_id": "gpt-oss-120b",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": DEFAULT_API_KEY,
        "base_url": DEFAULT_BASE_URL,
        "client": "openai",
    },
    "gpt-5-2025-08-07": {
        "model_id": "gpt-5",
        "temperature": 1,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": AZURE_API_KEY,
        "base_url": AZURE_BASE_URL,
        "api_version": AZURE_API_VERSION,
        "client": "azureopenai",
    },
    "claude-3-5-sonnet-20240620": {
        "model_id": "claude-3-5-sonnet-20240620",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": CLAUDE_API_KEY,
        "base_url": CLAUDE_BASE_URL,
        "client": "openai",
    },
    "claude-3-7-sonnet-20250219": {
        "model_id": "claude-3-7-sonnet-latest",
        "temperature": 0,
        "max_tokens": 512,
        "supports_system_prompt": True,
        "api_key": CLAUDE_API_KEY,
        "base_url": CLAUDE_BASE_URL,
        "client": "openai",
    },
    "claude-4-5-sonnet--20250929": {
        "model_id": "claude-sonnet-4-5-20250929",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": CLAUDE_API_KEY,
        "base_url": CLAUDE_BASE_URL,
        "client": "openai",
    },
    "gemini-2.0-flash": {
        "model_id": "gemini-2.0-flash",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": DEFAULT_API_KEY,
        "base_url": DEFAULT_BASE_URL,
        "client": "openai",
    },
    "gemini-2.5-flash": {
        "model_id": "gemini-2.5-flash",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": DEFAULT_API_KEY,
        "base_url": DEFAULT_BASE_URL,
        "client": "openai",
    },
    "gemini-2.5-pro": {
        "model_id": "gemini-2.5-pro",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": AZURE_API_KEY,
        "base_url": AZURE_BASE_URL,
        "api_version": "2024-03-01-preview",
        "client": "azureopenai",
    },
    "deepseek-chat": {
        "model_id": "deepseek-chat",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": DEFAULT_API_KEY,
        "base_url": DEFAULT_BASE_URL,
    },
    "deepseek-reasoner": {
        "model_id": "deepseek-reasoner",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": DEFAULT_API_KEY,
        "base_url": DEFAULT_BASE_URL,
    },
    "Llama-3.3-70B-Instruct": {
        "model_id": "Llama-3.3-70B-Instruct",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": LOCAL_API_KEY,
        "base_url": LOCAL_BASE_URL,
    },
    "Llama-3.1-70B-Instruct": {
        "model_id": "Llama-3.1-70B-Instruct",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": LOCAL_API_KEY,
        "base_url": LOCAL_BASE_URL,
    },
    "Llama-3.1-8B-Instruct": {
        "model_id": "Llama-3.1-8B-Instruct",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": LOCAL_API_KEY,
        "base_url": LOCAL_BASE_URL,
    },
    "webrl-llama-3.1-8b": {
        "model_id": "webrl-llama-3.1-8b",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": LOCAL_API_KEY,
        "base_url": LOCAL_BASE_URL,
    },
    "Qwen2.5-7B-Instruct": {
        "model_id": "Qwen2.5-7B-Instruct",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": LOCAL_API_KEY,
        "base_url": LOCAL_BASE_URL,
    },
    "Qwen2.5-32B-Instruct": {
        "model_id": "Qwen2.5-32B-Instruct",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": LOCAL_API_KEY,
        "base_url": LOCAL_BASE_URL,
    },
    "Qwen2.5-72B-Instruct": {
        "model_id": "Qwen2.5-72B-Instruct",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": LOCAL_API_KEY,
        "base_url": LOCAL_BASE_URL,
    },
    "Qwen2.5-VL-7B-Instruct": {
        "model_id": "Qwen2.5-VL-7B-Instruct",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": LOCAL_API_KEY,
        "base_url": LOCAL_BASE_URL,
    },
    "Qwen3-30B-A3B-Instruct-2507": {
        "model_id": "Qwen3-30B-A3B-Instruct-2507",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": LOCAL_API_KEY,
        "base_url": LOCAL_BASE_URL,
    },
    "UI-TARS-1.5-7B": {
        "model_id": "UI-TARS-1.5-7B",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": LOCAL_API_KEY,
        "base_url": LOCAL_BASE_URL,
    },
    "Qwen2.5-7B-ARPO": {
        "model_id": "Qwen2.5-7B-ARPO",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": LOCAL_API_KEY,
        "base_url": LOCAL_BASE_URL,
    },
    "grok-4-0709": {
        "model_id": "grok-4-0709",
        "temperature": 0,
        "max_tokens": 2048,
        "supports_system_prompt": True,
        "api_key": DEFAULT_API_KEY,
        "base_url": DEFAULT_BASE_URL,
    },
}


def get_model_config(model_name: str) -> Dict[str, Any]:
    """
    Get configuration for the specified model

    Args:
        model_name: Model name

    Returns:
        Model configuration dictionary

    Raises:
        ValueError: If the model does not exist
    """
    if model_name not in MODEL_CONFIGS:
        raise ValueError(
            f"Unknown model: {model_name}. Supported models: {', '.join(MODEL_CONFIGS.keys())}"
        )

    return MODEL_CONFIGS[model_name]


def get_client(model_name: str) -> Union[OpenAI, openai.AzureOpenAI]:
    """


    Args:
        model_name: Model name for retrieving the corresponding API configuration

    Returns:
        OpenAI or AzureOpenAI client instance

    Raises:
        EnvironmentError: If necessary API configuration is missing
    """
    # Get model configuration
    model_config = get_model_config(model_name)
    client_type = model_config.get("client", "openai")
    
    if client_type == "azureopenai":
        # AzureOpenAI specific configuration
        base_url = model_config.get("base_url")
        api_version = model_config.get("api_version")
        ak = model_config.get("api_key")
        
        print(f"Model {model_name} using AzureOpenAI with endpoint: {base_url}")
        return openai.AzureOpenAI(
            azure_endpoint=base_url,
            api_version=api_version,
            api_key=ak,
        )
    else:
        # Standard OpenAI client configuration
        api_key = model_config.get("api_key")
        base_url = model_config.get("base_url")

        client_args = {}
        if base_url:
            client_args["base_url"] = base_url.strip()
            print(f"Model {model_name} using API base URL: {base_url}")
        else:
            print(f"Model {model_name} using default OpenAI API endpoint")

        # Create client
        return OpenAI(api_key=api_key, **client_args)


def format_prompt(
    model_name: str,
    user_input: Union[str, List[Dict[str, Any]]],
    system_prompt: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Format prompt according to model

    Args:
        model_name: Model name
        user_input: User input content, can be a string or pre-formatted message list (for multimodal input)
        system_prompt: System prompt, some models may not support this

    Returns:
        Formatted prompt dictionary, can be used directly for API calls
    """
    model_config = get_model_config(model_name)
    supports_system_prompt = model_config.get("supports_system_prompt", True)

    # If user input is already a formatted message list (multimodal input), use it directly
    if isinstance(user_input, list):
        # In this case, the input is already a processed message list
        # system_prompt should have been handled in make_prompt and query_model
        return {"messages": user_input}

    # Process normal text input
    if supports_system_prompt and system_prompt:
        return {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input},
            ]
        }
    else:
        # For models that don't support system prompts, combine system prompt and user input
        if system_prompt:
            combined_input = f"{system_prompt}\n\n{user_input}"
        else:
            combined_input = user_input

        return {"messages": [{"role": "user", "content": combined_input}]}


def get_available_models() -> List[str]:
    """
    Get list of all available models

    Returns:
        List of model names
    """
    return list(MODEL_CONFIGS.keys())
